{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ffcd4c-222c-4240-9967-958738b6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b3addf-b2ad-412d-839b-c6f55f21fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Basic feature engineering\n",
    "data['Price_log'] = np.log1p(data['Price'])\n",
    "data['baths_beds_interaction'] = data['Baths'] * data['Beds']  # New interaction feature\n",
    "\n",
    "# Remove extreme outliers (top/bottom 1%)\n",
    "q_low = data['Price_log'].quantile(0.01)\n",
    "q_high = data['Price_log'].quantile(0.99)\n",
    "data = data[(data['Price_log'] > q_low) & (data['Price_log'] < q_high)]\n",
    "\n",
    "# Select features and target\n",
    "numerical_features = ['Baths', 'Beds', 'House size', 'baths_beds_interaction']  # Added interaction\n",
    "X = data[numerical_features]\n",
    "y = data['Price_log']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd101f90-2de2-47fb-bc12-8fdb83d83c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved as 'preprocessor.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define log transformer for House size\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('log', ColumnTransformer([\n",
    "        ('log_size', log_transformer, ['House size']),\n",
    "        ('passthrough', 'passthrough', ['Baths', 'Beds', 'baths_beds_interaction'])\n",
    "    ])),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('transformer', PowerTransformer())\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Save preprocessor to pickle file\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"Preprocessor saved as 'preprocessor.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bfd34fb-b5e0-4a60-be21-1a703e54d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 265.5157 - mae: 16.2363 - val_loss: 121.8010 - val_mae: 11.0111\n",
      "Epoch 2/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 85.4416 - mae: 8.9491 - val_loss: 4.9847 - val_mae: 2.1590\n",
      "Epoch 3/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8742 - mae: 1.4618 - val_loss: 0.2941 - val_mae: 0.3947\n",
      "Epoch 4/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3488 - mae: 0.4462 - val_loss: 0.3321 - val_mae: 0.4228\n",
      "Epoch 5/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2867 - mae: 0.4052 - val_loss: 0.2939 - val_mae: 0.3868\n",
      "Epoch 6/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2726 - mae: 0.3960 - val_loss: 0.2373 - val_mae: 0.3504\n",
      "Epoch 7/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2472 - mae: 0.3740 - val_loss: 0.2209 - val_mae: 0.3390\n",
      "Epoch 8/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2384 - mae: 0.3649 - val_loss: 0.2003 - val_mae: 0.3209\n",
      "Epoch 9/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2335 - mae: 0.3589 - val_loss: 0.2448 - val_mae: 0.3544\n",
      "Epoch 10/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2265 - mae: 0.3546 - val_loss: 0.2275 - val_mae: 0.3418\n",
      "Epoch 11/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2198 - mae: 0.3488 - val_loss: 0.2594 - val_mae: 0.3705\n",
      "Epoch 12/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2189 - mae: 0.3454 - val_loss: 0.2237 - val_mae: 0.3368\n",
      "Epoch 13/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2135 - mae: 0.3441 - val_loss: 0.2427 - val_mae: 0.3514\n",
      "Epoch 14/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2143 - mae: 0.3438 - val_loss: 0.2581 - val_mae: 0.3689\n",
      "Epoch 15/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2097 - mae: 0.3405 - val_loss: 0.2158 - val_mae: 0.3298\n",
      "Epoch 16/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2077 - mae: 0.3371 - val_loss: 0.2544 - val_mae: 0.3670\n",
      "Epoch 17/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2100 - mae: 0.3395 - val_loss: 0.2280 - val_mae: 0.3435\n",
      "Epoch 18/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2084 - mae: 0.3379 - val_loss: 0.2320 - val_mae: 0.3429\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Now 4 features\n",
    "    Dense(256, activation='relu'),  # Added layer\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67a9924-bebb-4231-a923-5de5122d4869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step \n",
      "Model Performance:\n",
      "RMSE: 0.4476\n",
      "R²: 0.6118\n"
     ]
    }
   ],
   "source": [
    "# Load best model weights\n",
    "model.load_weights('best_model.keras')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccecba-38ec-4f46-81ef-57b10be819a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
